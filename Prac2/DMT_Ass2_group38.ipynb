{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the needed packages\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null Handling\n",
    "gross_bookings_usd: 97% <br/>\n",
    "srch_query_affinity_score: 94%  <br/>\n",
    "orig_destination_distance: 32% #juul fixt deze <br/>\n",
    "prop_location_score2: 22% #replaced by mean <br/>\n",
    "prop_location_score1: 0% <br/>\n",
    "visitor_hist_adr_usd: 95% <br/>\n",
    "visitor_hist_starrating: 95% full corr. <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "expedia_df = pd.read_csv('training_set_VU_DM_2014.csv')\n",
    "test_df = pd.read_csv('test_set_VU_DM_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = expedia_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>prop_starrating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6938</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12605</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21958</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30512</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30572</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32045</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37487</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43935</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44063</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55984</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56517</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71269</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79848</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92326</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93644</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93977</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95291</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98064</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98368</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100806</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103689</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128807</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132164</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138629</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152440</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158263</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158980</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163696</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173356</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714603</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4717087</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4719139</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4722223</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736023</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747572</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748599</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759170</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4764397</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4817207</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831670</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841074</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4847247</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4850443</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4850858</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873142</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887979</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894467</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897109</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4900476</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4909145</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4909704</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4911069</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4913453</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4914707</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4914771</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916830</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929819</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933902</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951333</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         prop_review_score  prop_starrating\n",
       "0                      3.5                3\n",
       "6938                   3.5                3\n",
       "12605                  3.5                3\n",
       "21958                  3.5                3\n",
       "30512                  3.5                3\n",
       "30572                  3.5                3\n",
       "32045                  3.5                3\n",
       "37487                  3.5                3\n",
       "43935                  3.5                3\n",
       "44063                  3.5                3\n",
       "55984                  3.5                3\n",
       "56517                  3.5                3\n",
       "71269                  3.5                3\n",
       "79848                  3.5                3\n",
       "92326                  3.5                3\n",
       "93644                  3.5                3\n",
       "93977                  3.5                3\n",
       "95291                  3.5                3\n",
       "98064                  3.5                3\n",
       "98368                  3.5                3\n",
       "100806                 3.5                3\n",
       "103689                 3.5                3\n",
       "128807                 3.5                3\n",
       "132164                 3.5                3\n",
       "138629                 3.5                3\n",
       "152440                 3.5                3\n",
       "158263                 3.5                3\n",
       "158980                 3.5                3\n",
       "163696                 3.5                3\n",
       "173356                 3.5                3\n",
       "...                    ...              ...\n",
       "4714603                3.5                3\n",
       "4717087                3.5                3\n",
       "4719139                3.5                3\n",
       "4722223                3.5                3\n",
       "4736023                3.5                3\n",
       "4747572                3.5                3\n",
       "4748599                3.5                3\n",
       "4759170                3.5                3\n",
       "4764397                3.5                3\n",
       "4817207                3.5                3\n",
       "4831670                3.5                3\n",
       "4841074                3.5                3\n",
       "4847247                3.5                3\n",
       "4850443                3.5                3\n",
       "4850858                3.5                3\n",
       "4873142                3.5                3\n",
       "4887979                3.5                3\n",
       "4894467                3.5                3\n",
       "4897109                3.5                3\n",
       "4900476                3.5                3\n",
       "4909145                3.5                3\n",
       "4909704                3.5                3\n",
       "4911069                3.5                3\n",
       "4913453                3.5                3\n",
       "4914707                3.5                3\n",
       "4914771                3.5                3\n",
       "4916830                3.5                3\n",
       "4929819                3.5                3\n",
       "4933902                3.5                3\n",
       "4951333                3.5                3\n",
       "\n",
       "[612 rows x 2 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.loc[d['prop_id'] == 893][['prop_review_score','prop_starrating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_cont(data, variable_name, nbins):\n",
    "    labels_ = [str(x) for x in np.arange(nbins)]\n",
    "    # assign categorical labels to n bins\n",
    "    data[variable_name] = pd.cut(data[variable_name],nbins, labels=labels_)\n",
    "    # Transform categorical variable to several binary dummy variables\n",
    "    data = pd.get_dummies(data,columns=[variable_name])\n",
    "    return data\n",
    "    \n",
    "def normalize(data, variable):\n",
    "    d = data[variable]\n",
    "    mean = np.mean(d)\n",
    "    norm_d = [(x-mean)/mean for x in d]\n",
    "    data[variable] = norm_d\n",
    "    return data\n",
    "\n",
    "def pp_time(data):\n",
    "    data[\"date_time\"] = pd.to_datetime(data[\"date_time\"])\n",
    "    data[\"year\"] = data[\"date_time\"].dt.year\n",
    "    data[\"month\"] = data[\"date_time\"].dt.month\n",
    "    return data\n",
    "\n",
    "def binning(data):\n",
    "    data['price_usd'] =  pd.cut(data['price_usd'], bins=[0, 100, 175, 250, 500, 1000], include_lowest=True)\n",
    "    return data\n",
    "\n",
    "def join_comps(data):\n",
    "    comp_vars = ['comp1_rate','comp1_inv','comp1_rate_percent_diff',\n",
    "                'comp2_rate','comp2_inv','comp2_rate_percent_diff',\n",
    "                'comp3_rate','comp3_inv','comp3_rate_percent_diff',\n",
    "                'comp4_rate','comp4_inv','comp4_rate_percent_diff',\n",
    "                'comp5_rate','comp5_inv','comp5_rate_percent_diff',\n",
    "                'comp6_rate','comp6_inv','comp6_rate_percent_diff',\n",
    "                'comp7_rate','comp7_inv','comp7_rate_percent_diff',\n",
    "                'comp8_rate','comp8_inv','comp8_rate_percent_diff']\n",
    "    data = remove_comp_outliers(data, *[x for x in comp_vars if 'percent' in x])\n",
    "    data = combine_comps(data,comp_vars)\n",
    "    data.drop(comp_vars,axis=1)\n",
    "    \n",
    "    def remove_comp_outliers(data, *variables):\n",
    "        # Removes outliers from the percent_diff set. Only high-end outliers are removed\n",
    "        for var in variables:\n",
    "            median = data[var].median()\n",
    "            quantile = data[var].quantile(0.9)\n",
    "            thresh = median + (1.5*(quantile-median))\n",
    "            removed_outliers = []\n",
    "            for i,point in enumerate(data[var].values):\n",
    "                if not math.isnan(point):\n",
    "                    if point > thresh:\n",
    "                        removed_outliers.append(None)\n",
    "                    else:\n",
    "                        removed_outliers.append(point)\n",
    "                else:\n",
    "                    removed_outliers.append(None)\n",
    "            data[var] = removed_outliers\n",
    "        return data\n",
    "    def combine_comps(data, comp_vars):\n",
    "        def combine_rate_or_inv(row, is_inv):\n",
    "            if is_inv:\n",
    "                print('inv!',row)\n",
    "            non_null = [x for x in row if not math.isnan(x)]\n",
    "            if len(non_null) != 0:\n",
    "                return sum(non_null)/len(non_null)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        def combine_percent_diff(percent, signs):\n",
    "            rel_dif = np.array(percent) * np.array(signs)\n",
    "            rel_diff = [x for x in rel_dif if not math.isnan(x)]\n",
    "            if len(rel_diff) != 0:\n",
    "                return sum(rel_diff)/len(rel_diff)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        rate_vars = [x for x in comp_vars if 'rate' in x and 'percent' not in x]\n",
    "        inv_vars  = [x for x in comp_vars if 'inv'  in x]\n",
    "        percent_vars = [x for x in comp_vars if 'percent' in x]\n",
    "        print(rate_vars)\n",
    "        print(inv_vars)\n",
    "        print(percent_vars)\n",
    "        comp_rate = []\n",
    "        comp_inv = []\n",
    "        comp_diff = []\n",
    "        for i,row in enumerate(data[rate_vars].values):\n",
    "            comp_rate.append(combine_rate_or_inv(row,False))\n",
    "            comp_inv.append(combine_rate_or_inv(data[inv_vars].values[i],True))\n",
    "\n",
    "        for i,row in enumerate(data[percent_vars].values):\n",
    "            signs = data[rate_vars].values[i]\n",
    "            comp_diff.append(combine_percent_diff(row, signs))\n",
    "        data['comp_rate'] = comp_rate\n",
    "        data['comp_inv'] = comp_inv\n",
    "        data['comp_diff'] = comp_diff\n",
    "        print('Done')\n",
    "        return data\n",
    "    return data\n",
    "    \n",
    "def normalize_mult(data, *variables):\n",
    "    for var in variables:\n",
    "        data = normalize(data, var)\n",
    "    return data\n",
    "\n",
    "# TODO:\n",
    "# 1. Julian: Fix price (can be per night or per stay): differs per COUNTRY (either prop or src sure)\n",
    "# 2. Julian: Do orig_destination_distance & prop_location_score2 (correlation with loc1 and starrating)\n",
    "# 3. Julian: (after 1) Add price difference over history price_usd / e^prop_log_hist_price\n",
    "# 4. Normalize prop_starrating * prop_review_score over properties in query\n",
    "# 5. Normalize prop_starrating * prop_review_score over \n",
    "# 6. Replace null values with median (not sure for which) method change_null not implemened\n",
    "# Maybe: balance training set positive & negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = pp_time(data) # drops date time adds year and month\n",
    "    data = binning(data) # bins price_usd\n",
    "    data = join_comps(data) # drops compx vars adds joined comp vars (and outliers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.97% of total data sampled (n = 494304)\n",
      "10.01% of queries sampled\n",
      "Sampling took 0.48395419120788574 seconds\n"
     ]
    }
   ],
   "source": [
    "def create_train_file(data, name, features):\n",
    "    # This method converts the dataset to a txt format on which \n",
    "    # LambdaMART can be trained according to LEMUR file specification\n",
    "    # data: dataset\n",
    "    # name: name of train file\n",
    "    # features: list of feature names.\n",
    "    s = time.time()\n",
    "    def is_numerical(val):\n",
    "        if type(val) == float or type(val) == int:\n",
    "            return True\n",
    "        else:\n",
    "            for el in val:\n",
    "                if el != '.':\n",
    "                    if not el.isdigit():\n",
    "                        return False\n",
    "        return True\n",
    "    \n",
    "            \n",
    "    def feat_to_line(target, srch_id, feat_dict):\n",
    "        line = '{} qid:{} '.format(target, srch_id)\n",
    "        for (k,v) in list(feat_dict.items()):\n",
    "            assert is_numerical(v), 'non numerical value detected: {}'.format(v)\n",
    "            line += '{}:{} '.format(k,v)\n",
    "        return line\n",
    "    #f = open(name+'_train.txt','w')\n",
    "    for i, row in data.iterrows():\n",
    "        srch_id = row['srch_id']\n",
    "        target = max(np.array((row['booking_bool'],row['click_bool']))*[5,1])\n",
    "        feat_dict = {k+1:row[name] for k,name in enumerate(features)}\n",
    "        line = feat_to_line(target,srch_id, feat_dict)\n",
    "        print(line)\n",
    "        \n",
    "def sample_by_query(data, N):\n",
    "    s = time.time()\n",
    "    ids = data.srch_id.unique()\n",
    "    selection = np.random.choice(ids,N,replace=False)\n",
    "    result = data.loc[data['srch_id'].isin(selection)]\n",
    "    print('{0:.2f}% of total data sampled (n = {1:})'.format(result.shape[0]*100/data.shape[0], result.shape[0]))\n",
    "    print('{0:.2f}% of queries sampled'.format(N*100/len(ids)))\n",
    "    print('Sampling took {} seconds'.format(time.time()-s))\n",
    "    return result\n",
    "\n",
    "sampled_data = sample_by_query(expedia_df, 20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
