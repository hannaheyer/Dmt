{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the needed packages\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null Handling\n",
    "gross_bookings_usd: 97% <br/>\n",
    "srch_query_affinity_score: 94%  <br/>\n",
    "orig_destination_distance: 32% #juul fixt deze <br/>\n",
    "prop_location_score2: 22% #replaced by mean <br/>\n",
    "prop_location_score1: 0% <br/>\n",
    "visitor_hist_adr_usd: 95% <br/>\n",
    "visitor_hist_starrating: 95% full corr. <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "expedia_df = pd.read_csv('training_set_VU_DM_2014.csv')\n",
    "test_df = pd.read_csv('test_set_VU_DM_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>prop_log_historical_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>104.77</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>170.74</td>\n",
       "      <td>5.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>179.80</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>602.77</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>143.58</td>\n",
       "      <td>4.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>195.32</td>\n",
       "      <td>5.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>129.35</td>\n",
       "      <td>4.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>85.37</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>150.05</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>280.69</td>\n",
       "      <td>5.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>190.14</td>\n",
       "      <td>5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>100.89</td>\n",
       "      <td>4.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>100.89</td>\n",
       "      <td>4.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>210.84</td>\n",
       "      <td>5.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>138.40</td>\n",
       "      <td>5.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>100.89</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>152.63</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>138.40</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>115.12</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>191.44</td>\n",
       "      <td>5.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    srch_id  price_usd  prop_log_historical_price\n",
       "0         1     104.77                       4.95\n",
       "1         1     170.74                       5.03\n",
       "2         1     179.80                       4.92\n",
       "3         1     602.77                       4.39\n",
       "4         1     143.58                       4.93\n",
       "5         1     195.32                       5.20\n",
       "6         1     129.35                       4.81\n",
       "7         1      85.37                       4.14\n",
       "8         1     150.05                       5.18\n",
       "9         1     280.69                       5.15\n",
       "10        1     190.14                       5.08\n",
       "11        1     100.89                       4.78\n",
       "12        1     100.89                       4.44\n",
       "13        1     210.84                       5.03\n",
       "14        1     138.40                       5.03\n",
       "15        1     100.89                       4.50\n",
       "16        1     152.63                       5.18\n",
       "17        1     138.40                       4.80\n",
       "18        1     115.12                       4.98\n",
       "19        1     191.44                       5.28"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expedia_df[['srch_id','price_usd','prop_log_historical_price']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_cont(data, variable_name, nbins):\n",
    "    labels_ = [str(x) for x in np.arange(nbins)]\n",
    "    # assign categorical labels to n bins\n",
    "    data[variable_name] = pd.cut(data[variable_name],nbins, labels=labels_)\n",
    "    # Transform categorical variable to several binary dummy variables\n",
    "    data = pd.get_dummies(data,columns=[variable_name])\n",
    "    return data\n",
    "    \n",
    "\n",
    "def normalize(data, variable):\n",
    "    d = data[variable]\n",
    "    mean = np.mean(d)\n",
    "    norm_d = [(x-mean)/mean for x in d]\n",
    "    data[variable] = norm_d\n",
    "    return data\n",
    "\n",
    "def pp_time(data):\n",
    "    data[\"date_time\"] = pd.to_datetime(data[\"date_time\"])\n",
    "    data[\"year\"] = data[\"date_time\"].dt.year\n",
    "    data[\"month\"] = data[\"date_time\"].dt.month\n",
    "    return data\n",
    "\n",
    "def join_comps(data):\n",
    "    comp_vars = ['comp1_rate','comp1_inv','comp1_rate_percent_diff',\n",
    "                'comp2_rate','comp2_inv','comp2_rate_percent_diff',\n",
    "                'comp3_rate','comp3_inv','comp3_rate_percent_diff',\n",
    "                'comp4_rate','comp4_inv','comp4_rate_percent_diff',\n",
    "                'comp5_rate','comp5_inv','comp5_rate_percent_diff',\n",
    "                'comp6_rate','comp6_inv','comp6_rate_percent_diff',\n",
    "                'comp7_rate','comp7_inv','comp7_rate_percent_diff',\n",
    "                'comp8_rate','comp8_inv','comp8_rate_percent_diff']\n",
    "    data = remove_comp_outliers(data, *[x for x in comp_vars if 'percent' in x])\n",
    "    data = combine_comps(data,comp_vars)\n",
    "    data.drop(comp_vars,axis=1)\n",
    "    \n",
    "    def remove_comp_outliers(data, *variables):\n",
    "        # Removes outliers from the percent_diff set. Only high-end outliers are removed\n",
    "        for var in variables:\n",
    "            median = data[var].median()\n",
    "            quantile = data[var].quantile(0.9)\n",
    "            thresh = median + (1.5*(quantile-median))\n",
    "            removed_outliers = []\n",
    "            for i,point in enumerate(data[var].values):\n",
    "                if not math.isnan(point):\n",
    "                    if point > thresh:\n",
    "                        removed_outliers.append(None)\n",
    "                    else:\n",
    "                        removed_outliers.append(point)\n",
    "                else:\n",
    "                    removed_outliers.append(None)\n",
    "            data[var] = removed_outliers\n",
    "        return data\n",
    "    def combine_comps(data, comp_vars):\n",
    "        def combine_rate_or_inv(row, is_inv):\n",
    "            if is_inv:\n",
    "                print('inv!',row)\n",
    "            non_null = [x for x in row if not math.isnan(x)]\n",
    "            if len(non_null) != 0:\n",
    "                return sum(non_null)/len(non_null)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        def combine_percent_diff(percent, signs):\n",
    "            rel_dif = np.array(percent) * np.array(signs)\n",
    "            rel_diff = [x for x in rel_dif if not math.isnan(x)]\n",
    "            if len(rel_diff) != 0:\n",
    "                return sum(rel_diff)/len(rel_diff)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        rate_vars = [x for x in comp_vars if 'rate' in x and 'percent' not in x]\n",
    "        inv_vars  = [x for x in comp_vars if 'inv'  in x]\n",
    "        percent_vars = [x for x in comp_vars if 'percent' in x]\n",
    "        print(rate_vars)\n",
    "        print(inv_vars)\n",
    "        print(percent_vars)\n",
    "        comp_rate = []\n",
    "        comp_inv = []\n",
    "        comp_diff = []\n",
    "        for i,row in enumerate(data[rate_vars].values):\n",
    "            comp_rate.append(combine_rate_or_inv(row,False))\n",
    "            comp_inv.append(combine_rate_or_inv(data[inv_vars].values[i],True))\n",
    "\n",
    "        for i,row in enumerate(data[percent_vars].values):\n",
    "            signs = data[rate_vars].values[i]\n",
    "            comp_diff.append(combine_percent_diff(row, signs))\n",
    "        data['comp_rate'] = comp_rate\n",
    "        data['comp_inv'] = comp_inv\n",
    "        data['comp_diff'] = comp_diff\n",
    "        print('Done')\n",
    "        return data\n",
    "    return data\n",
    "    \n",
    "def normalize_mult(data, *variables):\n",
    "    for var in variables:\n",
    "        data = normalize(data, var)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = pp_time(data) # drops date time adds year, month\n",
    "    data = join_comps(data) # drops compx vars adds joined comp vars (and outliers)\n",
    "    data = normalize_mult(data, 'prop_rev')\n",
    "    data[var_name]= change_null(data,var_name) #fills nullvalues in variable with the mean \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
