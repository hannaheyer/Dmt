{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the needed packages\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Handling\n",
    "srch_query_affinity_score: 94% \n",
    "orig_destination_distance: 32% #juul fixt deze\n",
    "prop_location_score2: 22% #replaced by mean\n",
    "prop_location_score1: 0%\n",
    "visitor_hist_adr_usd: 95%\n",
    "visitor_hist_starrating: 95% full corr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "expedia_df = pd.read_csv('training_set_VU_DM_2014.csv')\n",
    "test_df = pd.read_csv('test_set_VU_DM_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling null values with mean\n",
    "def change_null(data, var_name):\n",
    "    data[var_name].fillna(data[var_name].mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, variable):\n",
    "    d = data[variable]\n",
    "    mean = np.mean(d)\n",
    "    norm_d = [(x-mean)/mean for x in d]\n",
    "    data[variable] = norm_d\n",
    "    return data\n",
    "\n",
    "def pp_time(data):\n",
    "    data[\"date_time\"] = pd.to_datetime(data[\"date_time\"])\n",
    "    data[\"year\"] = data[\"date_time\"].dt.year\n",
    "    data[\"month\"] = data[\"date_time\"].dt.month\n",
    "    return data\n",
    "\n",
    "def join_comps(data):\n",
    "    comp_vars = ['comp1_rate','comp1_inv','comp1_rate_percent_diff',\n",
    "                'comp2_rate','comp2_inv','comp2_rate_percent_diff',\n",
    "                'comp3_rate','comp3_inv','comp3_rate_percent_diff',\n",
    "                'comp4_rate','comp4_inv','comp4_rate_percent_diff',\n",
    "                'comp5_rate','comp5_inv','comp5_rate_percent_diff',\n",
    "                'comp6_rate','comp6_inv','comp6_rate_percent_diff',\n",
    "                'comp7_rate','comp7_inv','comp7_rate_percent_diff',\n",
    "                'comp8_rate','comp8_inv','comp8_rate_percent_diff']\n",
    "    data = remove_comp_outliers(data, *[x for x in comp_vars if 'percent' in x])\n",
    "    data = combine_comps(data,comp_vars)\n",
    "    data.drop(comp_vars,axis=1)\n",
    "    \n",
    "    def remove_comp_outliers(data, *variables):\n",
    "        # Removes outliers from the percent_diff set. Only high-end outliers are removed\n",
    "        for var in variables:\n",
    "            median = data[var].median()\n",
    "            quantile = data[var].quantile(0.9)\n",
    "            thresh = median + (1.5*(quantile-median))\n",
    "            removed_outliers = []\n",
    "            for i,point in enumerate(data[var].values):\n",
    "                if not math.isnan(point):\n",
    "                    if point > thresh:\n",
    "                        removed_outliers.append(None)\n",
    "                    else:\n",
    "                        removed_outliers.append(point)\n",
    "                else:\n",
    "                    removed_outliers.append(None)\n",
    "            data[var] = removed_outliers\n",
    "        return data\n",
    "    def combine_comps(data, comp_vars):\n",
    "        def combine_rate_or_inv(row, is_inv):\n",
    "            if is_inv:\n",
    "                print('inv!',row)\n",
    "            non_null = [x for x in row if not math.isnan(x)]\n",
    "            if len(non_null) != 0:\n",
    "                return sum(non_null)/len(non_null)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        def combine_percent_diff(percent, signs):\n",
    "            rel_dif = np.array(percent) * np.array(signs)\n",
    "            rel_diff = [x for x in rel_dif if not math.isnan(x)]\n",
    "            if len(rel_diff) != 0:\n",
    "                return sum(rel_diff)/len(rel_diff)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        rate_vars = [x for x in comp_vars if 'rate' in x and 'percent' not in x]\n",
    "        inv_vars  = [x for x in comp_vars if 'inv'  in x]\n",
    "        percent_vars = [x for x in comp_vars if 'percent' in x]\n",
    "        print(rate_vars)\n",
    "        print(inv_vars)\n",
    "        print(percent_vars)\n",
    "        comp_rate = []\n",
    "        comp_inv = []\n",
    "        comp_diff = []\n",
    "        for i,row in enumerate(data[rate_vars].values):\n",
    "            comp_rate.append(combine_rate_or_inv(row,False))\n",
    "            comp_inv.append(combine_rate_or_inv(data[inv_vars].values[i],True))\n",
    "\n",
    "        for i,row in enumerate(data[percent_vars].values):\n",
    "            signs = data[rate_vars].values[i]\n",
    "            comp_diff.append(combine_percent_diff(row, signs))\n",
    "        data['comp_rate'] = comp_rate\n",
    "        data['comp_inv'] = comp_inv\n",
    "        data['comp_diff'] = comp_diff\n",
    "        print('Done')\n",
    "        return data\n",
    "    return data\n",
    "    \n",
    "def normalize_mult(data, *variables):\n",
    "    for var in variables:\n",
    "        data = normalize(data, var)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = pp_time(data) # drops date time adds year, month\n",
    "    data = join_comps(data) # drops compx vars adds joined comp vars (and outliers)\n",
    "    data = normalize_mult(data, 'prop_rev')\n",
    "    data[var_name]= change_null(data,var_name) #fills nullvalues in variable with the mean \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING TODO'S\n",
    "# Add bin methods to preprocess func\n",
    "# Null values for origin_destination_distance\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
